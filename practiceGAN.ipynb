{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "practiceGAN",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akariiijima/GAN/blob/master/practiceGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBncyVSxEp8E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Activation, Reshape, Conv2D, Input\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers.convolutional import UpSampling2D, Convolution2D\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers import Flatten, Dropout\n",
        "import math\n",
        "import numpy as np\n",
        "import os\n",
        "from keras.datasets import mnist\n",
        "from keras.optimizers import Adam\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxf0aUgpEMwx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MnistGAN:\n",
        "  \n",
        "  def __init__(self):\n",
        "    self.z_dim = 100\n",
        "    self.shape = (28,28,1)\n",
        "    optimizer = Adam(lr=1e-4, beta_1=0.5)\n",
        "    \n",
        "    # build discriminator\n",
        "    self.discriminator = self.build_discriminator()\n",
        "    self.discriminator.compile(loss='binary_crossentropy', \\\n",
        "                               optimizer=optimizer, \\\n",
        "                               metrics=['acc'])\n",
        "    \n",
        "    # build generator \n",
        "    self.generator = self.build_generator()   \n",
        "    z = Input(shape=(self.z_dim,))\n",
        "    img = self.generator(z)\n",
        "    \n",
        "    # fix the parameters of discriminator\n",
        "    self.discriminator.trainable = False\n",
        "    valid = self.discriminator(img)\n",
        "    \n",
        "    # for training generator\n",
        "    self.combined = Model(z, valid)\n",
        "    self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "  \n",
        "  \n",
        "  def build_generator(self):\n",
        "    noise_shape = (self.z_dim,)    \n",
        "    model = Sequential()\n",
        "    \n",
        "    #-----------------------#\n",
        "    model.add(Dense(7*7*128, activation='relu', input_shape=noise_shape))\n",
        "    model.add(Reshape((7, 7, 128)))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(UpSampling2D())                   # (14,14,128)\n",
        "    #-----------------------#\n",
        "    model.add(Conv2D(8, kernel_size=3, padding='same'))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(UpSampling2D())                   # (28,28,128)\n",
        "    #-----------------------#\n",
        "    model.add(Conv2D(4, kernel_size=3, padding='same'))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNormalization(momentum=0.8)) # (28,28,64) \n",
        "    #-----------------------#\n",
        "    model.add(Conv2D(1, kernel_size=3, padding='same'))\n",
        "    model.add(Activation('tanh'))\n",
        "    \n",
        "    model.summary()\n",
        "    \n",
        "    noise = Input(shape=noise_shape)\n",
        "    \n",
        "    img = model(noise)\n",
        "    \n",
        "    return Model(inputs=noise, outputs=img)\n",
        "  \n",
        "  def build_discriminator(self):\n",
        "    img_shape = self.shape\n",
        "    \n",
        "    model = Sequential()\n",
        "    \n",
        "    model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=img_shape,\\\n",
        "                    padding='same'))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dropout(0.25))\n",
        "    #--------------------------------\n",
        "    model.add(Conv2D(64, kernel_size=3, strides=2, padding='same'))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dropout(0.25))\n",
        "    #--------------------------------\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.summary()\n",
        "    \n",
        "    img = Input(shape=img_shape)\n",
        "    validity = model(img)\n",
        "    \n",
        "    return Model(inputs=img, outputs=validity)\n",
        "  \n",
        "  def train(self, iterations, batch_size=128, save_interval=50, \\\n",
        "            model_interval=1000, check_noise=None, r=5, c=5):\n",
        "    x_train, y_train, x_test, y_test = self.load_imgs()\n",
        "    \n",
        "    half_batch = int(batch_size / 2)\n",
        "    \n",
        "    x_train = (x_train.astype(np.float32) - 127.5) / 127.5\n",
        "    \n",
        "    for iteration in range(iterations):\n",
        "      \"\"\"\n",
        "      Training Discriminator\n",
        "      \"\"\"\n",
        "      idx = np.random.randint(0, x_train.shape[0], half_batch)\n",
        "      \n",
        "      imgs = x_train[idx]\n",
        "      \n",
        "      noise = np.random.uniform(-1, 1, (half_batch, self.z_dim))\n",
        "      gen_imgs = self.generator.predict(noise)\n",
        "      \n",
        "      \n",
        "      d_loss_real = self.discriminator.train_on_batch(imgs, \\\n",
        "                                                      np.ones((half_batch, 1)))\n",
        "      d_loss_fake = self.discriminator.train_on_batch(gen_imgs, \\\n",
        "                                                      np.zeros((half_batch, 1)))\n",
        "    \n",
        "      d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "      \n",
        "      \"\"\"\n",
        "      Training Generator\n",
        "      \"\"\"\n",
        "      noise = np.random.uniform(-1, 1, (batch_size, self.z_dim))\n",
        "      \n",
        "      g_loss = self.combined.train_on_batch(noise, np.ones((batch_size, 1)))\n",
        "      \n",
        "      print(\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (iteration, d_loss[0], 100 * d_loss[1], g_loss))\n",
        "      for i in range(9):\n",
        "        plotImg = gen_imgs[i].reshape(28,28)\n",
        "        plt.subplot(3,3,i+1)\n",
        "        plt.imshow(plotImg, cmap='gray')\n",
        "      plt.show()\n",
        "      \n",
        "    \n",
        "  def load_imgs(self):\n",
        "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "    y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
        "    y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
        "    x_train = x_train.reshape((x_train.shape[0],x_train.shape[1],x_train.shape[2],1))\n",
        "    x_test = x_test.reshape((x_test.shape[0],x_test.shape[1],x_test.shape[2],1))\n",
        "    return x_train, y_train, x_test, y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bi5ctJAMEV8n",
        "colab_type": "code",
        "outputId": "71043425-369e-4149-8543-bcbce612bec6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "mnistGAN = MnistGAN()\n",
        "mnistGAN.train(iterations=3000)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}